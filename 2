import os
import shutil
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.utils import class_weight
from sklearn.metrics import confusion_matrix, balanced_accuracy_score, cohen_kappa_score
import seaborn as sns

# =========================
# 1) CONFIGURARE
# =========================
TRAIN_CSV_PATH = r"C:\Users\Jessyca\Downloads\aptos2019-blindness-detection\train.csv"
TRAIN_IMG_DIR  = r"C:\Users\Jessyca\Downloads\aptos2019-blindness-detection\train_images"
DATA_DIR = r"aptos_dataset"

IMG_SIZE = (224, 224)
BATCH_SIZE = 16
EPOCHS = 30
SEED = 123
BUILD_DATASET_FOLDERS = True

# =========================
# 2) GPU
# =========================
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# =========================
# 3) BUILD DATASET FOLDERS
# =========================
def ensure_dirs():
    for split in ["train", "val", "test"]:
        for c in range(5):
            os.makedirs(os.path.join(DATA_DIR, split, str(c)), exist_ok=True)

def build_folders_from_csv():
    print(" Construiesc structura APTOS...")
    df = pd.read_csv(TRAIN_CSV_PATH)
    df["path"] = df["id_code"].apply(lambda x: os.path.join(TRAIN_IMG_DIR, f"{x}.png"))
    df["diagnosis"] = df["diagnosis"].astype(int)

    train_df, temp_df = train_test_split(
        df, test_size=0.2, random_state=SEED, stratify=df["diagnosis"]
    )
    val_df, test_df = train_test_split(
        temp_df, test_size=0.5, random_state=SEED, stratify=temp_df["diagnosis"]
    )

    ensure_dirs()

    def copy_split(split_df, split_name):
        missing = 0
        for _, row in split_df.iterrows():
            src = row["path"]
            label = str(int(row["diagnosis"]))
            if not os.path.exists(src):
                missing += 1
                continue
            dst = os.path.join(DATA_DIR, split_name, label, os.path.basename(src))
            if not os.path.exists(dst):
                shutil.copy2(src, dst)
        if missing:
            print(f" Lipsă {missing} imagini la split={split_name}")

    copy_split(train_df, "train")
    copy_split(val_df, "val")
    copy_split(test_df, "test")

if BUILD_DATASET_FOLDERS:
    ensure_dirs()
    has_files = False
    for _, _, files in os.walk(os.path.join(DATA_DIR, "train")):
        if files:
            has_files = True
            break
    if not has_files:
        build_folders_from_csv()
    else:
        print(" Dataset deja pregătit.")

# =========================
# 4) LOAD DATA
# =========================
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    os.path.join(DATA_DIR, "train"),
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode="int",
    shuffle=True,
    seed=SEED
)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    os.path.join(DATA_DIR, "val"),
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode="int",
    shuffle=False
)

test_ds = tf.keras.preprocessing.image_dataset_from_directory(
    os.path.join(DATA_DIR, "test"),
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode="int",
    shuffle=False
)

class_names = train_ds.class_names
print("Clase:", class_names)

# =========================
# 5) CLASS WEIGHTS
# =========================
y_train_all = np.array([int(y.numpy()) for _, y in train_ds.unbatch()])
weights = class_weight.compute_class_weight(
    class_weight="balanced",
    classes=np.unique(y_train_all),
    y=y_train_all
)
class_weights = {int(c): float(w) for c, w in zip(np.unique(y_train_all), weights)}
print("Class weights:", class_weights)

# =========================
# 6) PREFETCH
# =========================
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.shuffle(500).prefetch(AUTOTUNE)
val_ds = val_ds.prefetch(AUTOTUNE)
test_ds = test_ds.prefetch(AUTOTUNE)

# =========================
# 7) DATA AUGMENTATION + PREPROCESS RESNET50
# =========================
# ResNet50 vrea preprocess_input (scale + mean subtraction). NU mai folosim Rescaling(1./255).
data_augmentation = keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.05),
    layers.RandomZoom(0.1),
], name="aug")

def resnet_preprocess(x, y, training=False):
    x = tf.cast(x, tf.float32)
    if training:
        x = data_augmentation(x, training=True)
    x = preprocess_input(x)  # IMPORTANT pentru ResNet50
    return x, y

train_ds_pp = train_ds.map(lambda x, y: resnet_preprocess(x, y, training=True), num_parallel_calls=AUTOTUNE)
val_ds_pp   = val_ds.map(lambda x, y: resnet_preprocess(x, y, training=False), num_parallel_calls=AUTOTUNE)
test_ds_pp  = test_ds.map(lambda x, y: resnet_preprocess(x, y, training=False), num_parallel_calls=AUTOTUNE)

train_ds_pp = train_ds_pp.prefetch(AUTOTUNE)
val_ds_pp   = val_ds_pp.prefetch(AUTOTUNE)
test_ds_pp  = test_ds_pp.prefetch(AUTOTUNE)

# =========================
# 8) MODEL – RESNET50
# =========================
base_model = ResNet50(
    weights="imagenet",
    include_top=False,
    input_shape=IMG_SIZE + (3,)
)
base_model.trainable = False

inputs = keras.Input(shape=IMG_SIZE + (3,))
x = base_model(inputs, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(5, activation="softmax")(x)
model = keras.Model(inputs, outputs)

model.compile(
    optimizer=keras.optimizers.Adam(1e-4),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

model.summary()

# =========================
# 9) TRAIN
# =========================
early_stop = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    patience=5,
    restore_best_weights=True
)

history = model.fit(
    train_ds_pp,
    validation_data=val_ds_pp,
    epochs=EPOCHS,
    class_weight=class_weights,
    callbacks=[early_stop]
)

# =========================
# 10) PLOT
# =========================
acc = history.history["accuracy"]
val_acc = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]

plt.figure(figsize=(14,5))
plt.subplot(1,2,1)
plt.plot(acc, label="Train Acc")
plt.plot(val_acc, label="Val Acc")
plt.legend(); plt.grid()

plt.subplot(1,2,2)
plt.plot(loss, label="Train Loss")
plt.plot(val_loss, label="Val Loss")
plt.legend(); plt.grid()
plt.show()

# =========================
# 11) TEST
# =========================
y_true, y_pred = [], []
for imgs, labels in test_ds_pp:
    preds = model.predict(imgs, verbose=0)
    y_pred.extend(np.argmax(preds, axis=1))
    y_true.extend(labels.numpy())

y_true = np.array(y_true)
y_pred = np.array(y_pred)

bal_acc = balanced_accuracy_score(y_true, y_pred)
qwk = cohen_kappa_score(y_true, y_pred, weights="quadratic")

print(f" Balanced Accuracy: {bal_acc:.4f}")
print(f" QWK: {qwk:.4f}")

cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(7,6))
sns.heatmap(cm, annot=True, fmt="d",
            xticklabels=class_names,
            yticklabels=class_names,
            cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicție")
plt.ylabel("Realitate")
plt.show()

# =========================
# 12) SAVE
# =========================
model.save("aptos_resnet50_final.keras")
print(" Model salvat: aptos_resnet50_final.keras")
